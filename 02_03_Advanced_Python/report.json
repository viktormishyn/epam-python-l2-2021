{
    "numberOfCharacters": 1355,
    "numberOfWords": 278,
    "numberOfSentences": 20,
    "frequencyOfCharacters": {
        "e": 148,
        "t": 132,
        "n": 111,
        "a": 108,
        "o": 103,
        "i": 97,
        "s": 74,
        "r": 70,
        "l": 56,
        "h": 44,
        "u": 41,
        "c": 37,
        "d": 35,
        "g": 30,
        "w": 27,
        "m": 25,
        "f": 23,
        "k": 21,
        "y": 17,
        "p": 16,
        ".": 15,
        "b": 13,
        "P": 11,
        ",": 10,
        "z": 10,
        "x": 8,
        "v": 8,
        "N": 8,
        "L": 8,
        "-": 7,
        "T": 6,
        "?": 5,
        "W": 5,
        "A": 4,
        "I": 3,
        "\u2019": 3,
        "q": 3,
        "S": 2,
        ":": 2,
        "'": 1,
        "M": 1,
        "(": 1,
        ")": 1,
        "C": 1,
        "\u2013": 1,
        "B": 1,
        "\u2018": 1,
        "E": 1
    },
    "distributionOfCharacters": {
        "e": "10.92 %",
        "t": "9.74 %",
        "n": "8.19 %",
        "a": "7.97 %",
        "o": "7.60 %",
        "i": "7.16 %",
        "s": "5.46 %",
        "r": "5.17 %",
        "l": "4.13 %",
        "h": "3.25 %",
        "u": "3.03 %",
        "c": "2.73 %",
        "d": "2.58 %",
        "g": "2.21 %",
        "w": "1.99 %",
        "m": "1.85 %",
        "f": "1.70 %",
        "k": "1.55 %",
        "y": "1.25 %",
        "p": "1.18 %",
        ".": "1.11 %",
        "b": "0.96 %",
        "P": "0.81 %",
        ",": "0.74 %",
        "z": "0.74 %",
        "x": "0.59 %",
        "v": "0.59 %",
        "N": "0.59 %",
        "L": "0.59 %",
        "-": "0.52 %",
        "T": "0.44 %",
        "?": "0.37 %",
        "W": "0.37 %",
        "A": "0.30 %",
        "I": "0.22 %",
        "\u2019": "0.22 %",
        "q": "0.22 %",
        "S": "0.15 %",
        ":": "0.15 %",
        "'": "0.07 %",
        "M": "0.07 %",
        "(": "0.07 %",
        ")": "0.07 %",
        "C": "0.07 %",
        "\u2013": "0.07 %",
        "B": "0.07 %",
        "\u2018": "0.07 %",
        "E": "0.07 %"
    },
    "avgWordLength": 4.73,
    "avgNumberOfWordsInSentence": 13.9,
    "topMostUsedWords": {
        "the": 11,
        "to": 9,
        "tokenization": 8,
        "of": 7,
        "text": 7,
        "data": 7,
        "in": 7,
        "we": 6,
        "nlp": 6,
        "and": 5
    },
    "topLongestWords": {
        "lemmatization": 13,
        "prerequisites": 13,
        "comprehensive": 13,
        "unstructured": 12,
        "tokenization": 12,
        "multi-stage": 11,
        "identifying": 11,
        "essentially": 11,
        "fascinated": 10,
        "manipulate": 10
    },
    "topShortestWords": {
        "a": 1,
        "-": 1,
        "by": 2,
        "of": 2,
        "on": 2,
        "to": 2,
        "be": 2,
        "in": 2,
        "so": 2,
        "we": 2
    },
    "topLongestSentences": {
        "Cleaning the data consists of a few key steps: - Word tokenization - Predicting parts of speech for each token - Text lemmatization - Identifying and removing stop words, and much more.": 32,
        "If you\u2019re looking for an end-to-end resource for learning NLP, you should check out our comprehensive course: Natural Language Processing using Python.": 22,
        "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms.": 22,
        "We need to clean the unstructured text data first before we can even think about getting to the modeling stage.": 20,
        "Are you looking for ways to work with this text data but aren't sure where to begin?": 17,
        "Tokenization is one of the most common tasks when it comes to working with text data.": 16,
        "Anyone with an interest in NLP or data science will be able to follow along.": 15,
        "So how can we manipulate and clean this text data to build a model?": 14,
        "Are you fascinated by the amount of text data available on the internet?": 13,
        "We will first see what tokenization is and why it\u2019s required in NLP.": 13
    },
    "topShortestSentences": {
        "This article has no prerequisites.": 5,
        "What is Tokenization in NLP?": 5,
        "Solving an NLP problem is a multi-stage process.": 8,
        "But what does the term \u2018tokenization\u2019 actually mean?": 8,
        "Each of these smaller units are called tokens.": 8,
        "Machines, after all, recognize numbers, not the letters of our language.": 11,
        "And that can be a tricky landscape to navigate in machine learning.": 12,
        "The answer lies in the wonderful world of Natural Language Processing (NLP).": 12,
        "In this article, we will talk about the very first step \u2013 tokenization.": 12,
        "Are you fascinated by the amount of text data available on the internet?": 13
    },
    "numberOfPalindromes": 0,
    "topLongestPalindromes": [],
    "isTextPalindrome": false,
    "reportGeneratedAt": "2021-10-16 21:14:58.246168",
    "timeOfExecution": "0.0010256030000164174 ms"
}
[
    {
        "filename": "data/text1.txt",
        "numberOfCharacters": 1355,
        "numberOfWords": 274,
        "numberOfSentences": 20,
        "frequencyOfCharacters": {
            "e": 148,
            "t": 132,
            "n": 111,
            "a": 108,
            "o": 103,
            "i": 97,
            "s": 74,
            "r": 70,
            "l": 56,
            "h": 44,
            "u": 41,
            "c": 37,
            "d": 35,
            "g": 30,
            "w": 27,
            "m": 25,
            "f": 23,
            "k": 21,
            "y": 17,
            "p": 16,
            ".": 15,
            "b": 13,
            "P": 11,
            ",": 10,
            "z": 10,
            "x": 8,
            "v": 8,
            "N": 8,
            "L": 8,
            "-": 7,
            "T": 6,
            "?": 5,
            "W": 5,
            "A": 4,
            "I": 3,
            "\u2019": 3,
            "q": 3,
            "S": 2,
            ":": 2,
            "'": 1,
            "M": 1,
            "(": 1,
            ")": 1,
            "C": 1,
            "\u2013": 1,
            "B": 1,
            "\u2018": 1,
            "E": 1
        },
        "distributionOfCharacters": {
            "e": "10.92 %",
            "t": "9.74 %",
            "n": "8.19 %",
            "a": "7.97 %",
            "o": "7.60 %",
            "i": "7.16 %",
            "s": "5.46 %",
            "r": "5.17 %",
            "l": "4.13 %",
            "h": "3.25 %",
            "u": "3.03 %",
            "c": "2.73 %",
            "d": "2.58 %",
            "g": "2.21 %",
            "w": "1.99 %",
            "m": "1.85 %",
            "f": "1.70 %",
            "k": "1.55 %",
            "y": "1.25 %",
            "p": "1.18 %",
            ".": "1.11 %",
            "b": "0.96 %",
            "P": "0.81 %",
            ",": "0.74 %",
            "z": "0.74 %",
            "x": "0.59 %",
            "v": "0.59 %",
            "N": "0.59 %",
            "L": "0.59 %",
            "-": "0.52 %",
            "T": "0.44 %",
            "?": "0.37 %",
            "W": "0.37 %",
            "A": "0.30 %",
            "I": "0.22 %",
            "\u2019": "0.22 %",
            "q": "0.22 %",
            "S": "0.15 %",
            ":": "0.15 %",
            "'": "0.07 %",
            "M": "0.07 %",
            "(": "0.07 %",
            ")": "0.07 %",
            "C": "0.07 %",
            "\u2013": "0.07 %",
            "B": "0.07 %",
            "\u2018": "0.07 %",
            "E": "0.07 %"
        },
        "avgWordLength": 4.78,
        "avgNumberOfWordsInSentence": 13.7,
        "topMostUsedWords": {
            "the": 11,
            "to": 9,
            "tokenization": 8,
            "of": 7,
            "text": 7,
            "data": 7,
            "in": 7,
            "we": 6,
            "nlp": 6,
            "and": 5
        },
        "topLongestWords": [
            "lemmatization",
            "prerequisites",
            "comprehensive",
            "unstructured",
            "tokenization",
            "multi-stage",
            "identifying",
            "essentially",
            "fascinated",
            "manipulate"
        ],
        "topShortestWords": [
            "a",
            "by",
            "of",
            "on",
            "to",
            "be",
            "in",
            "so",
            "we",
            "an"
        ],
        "topLongestSentences": [
            "Cleaning the data consists of a few key steps: - Word tokenization - Predicting parts of speech for each token - Text lemmatization - Identifying and removing stop words, and much more.",
            "If you\u2019re looking for an end-to-end resource for learning NLP, you should check out our comprehensive course: Natural Language Processing using Python.",
            "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms.",
            "We need to clean the unstructured text data first before we can even think about getting to the modeling stage.",
            "Are you looking for ways to work with this text data but aren't sure where to begin?",
            "Tokenization is one of the most common tasks when it comes to working with text data.",
            "Anyone with an interest in NLP or data science will be able to follow along.",
            "So how can we manipulate and clean this text data to build a model?",
            "Are you fascinated by the amount of text data available on the internet?",
            "We will first see what tokenization is and why it\u2019s required in NLP."
        ],
        "topShortestSentences": [
            "This article has no prerequisites.",
            "What is Tokenization in NLP?",
            "Solving an NLP problem is a multi-stage process.",
            "But what does the term \u2018tokenization\u2019 actually mean?",
            "Each of these smaller units are called tokens.",
            "Machines, after all, recognize numbers, not the letters of our language.",
            "And that can be a tricky landscape to navigate in machine learning.",
            "The answer lies in the wonderful world of Natural Language Processing (NLP).",
            "In this article, we will talk about the very first step \u2013 tokenization.",
            "Are you fascinated by the amount of text data available on the internet?"
        ],
        "numberOfPalindromes": 0,
        "topLongestPalindromes": [],
        "isTextPalindrome": false,
        "reportGeneratedAt": "2022-01-08 11:53:12.162603",
        "timeOfExecution": "-0.0074774199997591495 ms"
    }
]
